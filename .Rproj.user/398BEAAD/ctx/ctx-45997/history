options(
digits = 6, # Significant figures output
scipen = 999, # Disable scientific notation
repos = getOption("repos")["CRAN"] # Install packages from CRAN
)
Sys.setenv(
MAKEFLAGS = paste0(
"-j",
parallel::detectCores(logical = FALSE)
))
{
## Check for existing installations
stan_packages <- installed.packages()[
grepl("cmdstanr|rstan$|StanHeaders|brms$",
installed.packages()[, 1]), 1]
## Remove any existing Stan packages
if (length(stan_packages) > 0) {
remove.packages(c("StanHeaders", "rstan", "brms"))
}
## Delete any pre-existing RData file
if (file.exists(".RData")) {
file.remove(".RData")
}
}
{
## Retrieve installed packages
pkgs <- installed.packages()[, 1]
## Check if rstudioapi is installed
if (isTRUE(all.equal(grep("rstudioapi", pkgs), integer(0)))) {
print("Installing the {rstudioapi} package")
install.packages("rstudioapi")
}
## Check if remotes is installed
if (isTRUE(all.equal(grep("remotes", pkgs), integer(0)))) {
print("Installing the {remotes} package")
install.packages("remotes")
}
## Else print a message
else {
print("{remotes} and {rstudioapi} packages are already installed")
}
}
install.packages(
pkgs = "rstan",
repos = c(
"https://mc-stan.org/r-packages/",
getOption("repos")
))
example(stan_model, package = "rstan", run.dontrun = TRUE)
install.packages(
pkgs = "rstan",
repos = c(
"https://mc-stan.org/r-packages/",
getOption("repos")
))
install.packages(c( "pacman", "remotes", "tidyverse", "data.table", "dtplyr", "downlit", "xml2", "rmarkdown", "sessioninfo", "brms" ))
install.packages("palmerpenguins")
install.packages("quarto")
colourpicker:::plotHelperAddin()
install.packages("htmlwidgets")
colourpicker:::colourPickerAddin()
colourpicker:::plotHelperAddin()
install.packages("shiny")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
set.seed(42)
library(dplyr)
library(purrr)
library(ggplot2)
library(readr)
dir.create("outputs/figures", recursive = TRUE, showWarnings = FALSE)
clip_prob <- function(p, eps = 1e-6) pmax(pmin(p, 1 - eps), eps)
sigmoid <- function(x) 1 / (1 + exp(-x))
normalize_log_weights <- function(logw) {
m <- max(logw)
w <- exp(logw - m)
w / sum(w)
}
opp_model_random_beta <- function(a0 = 1, b0 = 1, rho = 0.98) {
# Leaky Beta-Bernoulli counts for P(opp=1)
state <- list(a = a0, b = b0, a0 = a0, b0 = b0, rho = rho)
list(
name = "OppRandomBeta",
init = function() { state$a <- state$a0; state$b <- state$b0; invisible(state) },
predict_p1 = function(role_self, role_opp) clip_prob(state$a / (state$a + state$b)),
learn = function(opp_action, own_action, role_self, role_opp, payoff_opp) {
state$a <- state$a0 + state$rho * (state$a - state$a0) + opp_action
state$b <- state$b0 + state$rho * (state$b - state$b0) + (1 - opp_action)
invisible(state)
}
)
}
opp_model_wsls <- function(lapse = 0.10) {
# Opponent-as-WSLS with lapse
state <- list(prev_a = NA_integer_, prev_win = NA_integer_, lapse = lapse)
list(
name = "OppWSLS",
init = function() { state$prev_a <- NA_integer_; state$prev_win <- NA_integer_; invisible(state) },
predict_p1 = function(role_self, role_opp) {
if (is.na(state$prev_a)) return(0.5)
target <- if (state$prev_win == 1L) state$prev_a else (1L - state$prev_a)
p <- (1 - state$lapse) * as.numeric(target == 1L) + state$lapse * 0.5
clip_prob(p)
},
learn = function(opp_action, own_action, role_self, role_opp, payoff_opp) {
state$prev_a <- opp_action
state$prev_win <- as.integer(payoff_opp == 1L)
invisible(state)
}
)
}
opp_model_memory <- function(alpha = 0.15, tau = 8, lapse = 0.05) {
# Opponent tracks OUR action bias with leaky integration, then best-responds
state <- list(m = 0.5, alpha = alpha, tau = tau, lapse = lapse)
list(
name = "OppMemory",
init = function() { state$m <- 0.5; invisible(state) },
predict_p1 = function(role_self, role_opp) {
x <- if (role_opp == "matcher") (state$m - 0.5) else (0.5 - state$m)
p1 <- (1 - state$lapse) * sigmoid(state$tau * x) + state$lapse * 0.5
clip_prob(p1)
},
learn = function(opp_action, own_action, role_self, role_opp, payoff_opp) {
state$m <- state$m + state$alpha * (own_action - state$m)
invisible(state)
}
)
}
new_wsls_agent <- function(lapse = 0.05) {
state <- list(prev_a = NA_integer_, prev_win = NA_integer_, lapse = lapse)
list(
name = sprintf("WSLS(lapse=%.2f)", lapse),
init = function() { state$prev_a <- NA_integer_; state$prev_win <- NA_integer_; invisible(state) },
act = function(role_self, role_opp) {
if (is.na(state$prev_a)) return(sample(c(0L, 1L), 1))
if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))
if (state$prev_win == 1L) state$prev_a else (1L - state$prev_a)
},
learn = function(own_action, opp_action, payoff, role_self, role_opp) {
state$prev_a <- own_action
state$prev_win <- as.integer(payoff == 1L)
invisible(state)
}
)
}
new_memory_agent <- function(alpha = 0.15, tau = 8, lapse = 0.05) {
# Tracks opponent bias: m <- m + alpha*(opp - m)
state <- list(m = 0.5, alpha = alpha, tau = tau, lapse = lapse)
list(
name = sprintf("Memory(alpha=%.2f,tau=%.1f,lapse=%.2f)", alpha, tau, lapse),
init = function() { state$m <- 0.5; invisible(state) },
act = function(role_self, role_opp) {
if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))
x <- if (role_self == "matcher") (state$m - 0.5) else (0.5 - state$m)
rbinom(1, 1, clip_prob(sigmoid(state$tau * x)))
},
learn = function(own_action, opp_action, payoff, role_self, role_opp) {
state$m <- state$m + state$alpha * (opp_action - state$m)
invisible(state)
}
)
}
new_mixture_inference_agent <- function(
models = NULL,
lambda_logw = 0.98,
tau = 10,
lapse = 0.03
) {
# Online model averaging over opponent models (ambitious strategy)
if (is.null(models)) {
models <- list(
opp_model_random_beta(a0 = 1, b0 = 1, rho = 0.98),
opp_model_wsls(lapse = 0.10),
opp_model_memory(alpha = 0.15, tau = 8, lapse = 0.05)
)
}
state <- list(logw = rep(0, length(models)), lambda_logw = lambda_logw, tau = tau, lapse = lapse)
list(
name = "MixtureInference",
init = function() { state$logw <- rep(0, length(models)); for (m in models) m$init(); invisible(state) },
act = function(role_self, role_opp) {
if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))
p_each <- vapply(models, function(m) m$predict_p1(role_self, role_opp), numeric(1))
w <- normalize_log_weights(state$logw)
p_mix <- clip_prob(sum(w * p_each))
x <- if (role_self == "matcher") (p_mix - 0.5) else (0.5 - p_mix)
rbinom(1, 1, clip_prob(sigmoid(state$tau * x)))
},
learn = function(own_action, opp_action, payoff, role_self, role_opp) {
payoff_opp <- -payoff
p_each <- vapply(models, function(m) m$predict_p1(role_self, role_opp), numeric(1))
p_each <- clip_prob(p_each)
loglik <- if (opp_action == 1L) log(p_each) else log(1 - p_each)
state$logw <- state$lambda_logw * state$logw + loglik
for (m in models) m$learn(opp_action, own_action, role_self, role_opp, payoff_opp)
invisible(state)
}
)
}
new_random_agent <- function(theta = 0.5) {
list(
name = sprintf("Random(theta=%.2f)", theta),
init = function() invisible(NULL),
act = function(role_self, role_opp) rbinom(1, 1, theta),
learn = function(own_action, opp_action, payoff, role_self, role_opp) invisible(NULL)
)
}
simulate_match <- function(agent_A, agent_B, n_trials = 60, swap_at = 30) {
stopifnot(swap_at >= 1, swap_at <= n_trials)
agent_A$init()
agent_B$init()
role_A <- c(rep("matcher", swap_at), rep("hider", n_trials - swap_at))
role_B <- ifelse(role_A == "matcher", "hider", "matcher")
out <- vector("list", n_trials)
for (t in seq_len(n_trials)) {
a <- agent_A$act(role_self = role_A[t], role_opp = role_B[t])
b <- agent_B$act(role_self = role_B[t], role_opp = role_A[t])
payoff_A <- if (role_A[t] == "matcher") {
if (a == b) 1L else -1L
} else {
if (a != b) 1L else -1L
}
payoff_B <- -payoff_A
agent_A$learn(own_action = a, opp_action = b, payoff = payoff_A, role_self = role_A[t], role_opp = role_B[t])
agent_B$learn(own_action = b, opp_action = a, payoff = payoff_B, role_self = role_B[t], role_opp = role_A[t])
out[[t]] <- data.frame(
trial = t,
role_A = role_A[t],
role_B = role_B[t],
action_A = a,
action_B = b,
payoff_A = payoff_A,
payoff_B = payoff_B
)
}
bind_rows(out)
}
make_ordered_pairs <- function(names) {
expand.grid(A = names, B = names, stringsAsFactors = FALSE) |>
filter(A != B) |>
split(seq_len(nrow(.))) |>
lapply(\(r) list(A = r$A, B = r$B))
}
run_tournament <- function(agent_factories, n_sims = 300, n_trials = 60, swap_at = 30, seed = 1) {
set.seed(seed)
strategies <- names(agent_factories)
pairs <- make_ordered_pairs(strategies)
map_dfr(pairs, function(pair) {
Aname <- pair$A
Bname <- pair$B
pair_label <- paste0(Aname, " vs ", Bname)
map_dfr(seq_len(n_sims), function(sim_id) {
A <- agent_factories[[Aname]]()
B <- agent_factories[[Bname]]()
d <- simulate_match(A, B, n_trials = n_trials, swap_at = swap_at)
d$sim <- sim_id
d$pair <- pair_label
d$strategy_A <- Aname
d$strategy_B <- Bname
d
})
})
}
agent_factories <- list(
WSLS   = function() new_wsls_agent(lapse = 0.05),
Memory = function() new_memory_agent(alpha = 0.15, tau = 8, lapse = 0.05),
Mixture= function() new_mixture_inference_agent(lambda_logw = 0.98, tau = 10, lapse = 0.03),
Random = function() new_random_agent(theta = 0.50)
)
df <- run_tournament(agent_factories, n_sims = 300, n_trials = 60, swap_at = 30, seed = 42)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
set.seed(123)
library(readr)
library(dplyr)
# Optional (only needed for this file)
library(cmdstanr)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
set.seed(42)
library(dplyr)
library(purrr)
library(ggplot2)
library(readr)
dir.create("outputs/figures", recursive = TRUE, showWarnings = FALSE)
clip_prob <- function(p, eps = 1e-6) pmax(pmin(p, 1 - eps), eps)
sigmoid <- function(x) 1 / (1 + exp(-x))
normalize_log_weights <- function(logw) {
m <- max(logw)
w <- exp(logw - m)
w / sum(w)
}
opp_model_random_beta <- function(a0 = 1, b0 = 1, rho = 0.98) {
# Leaky Beta-Bernoulli counts for P(opp=1)
state <- list(a = a0, b = b0, a0 = a0, b0 = b0, rho = rho)
list(
name = "OppRandomBeta",
init = function() { state$a <- state$a0; state$b <- state$b0; invisible(state) },
predict_p1 = function(role_self, role_opp) clip_prob(state$a / (state$a + state$b)),
learn = function(opp_action, own_action, role_self, role_opp, payoff_opp) {
state$a <- state$a0 + state$rho * (state$a - state$a0) + opp_action
state$b <- state$b0 + state$rho * (state$b - state$b0) + (1 - opp_action)
invisible(state)
}
)
}
opp_model_wsls <- function(lapse = 0.10) {
# Opponent-as-WSLS with lapse
state <- list(prev_a = NA_integer_, prev_win = NA_integer_, lapse = lapse)
list(
name = "OppWSLS",
init = function() { state$prev_a <- NA_integer_; state$prev_win <- NA_integer_; invisible(state) },
predict_p1 = function(role_self, role_opp) {
if (is.na(state$prev_a)) return(0.5)
target <- if (state$prev_win == 1L) state$prev_a else (1L - state$prev_a)
p <- (1 - state$lapse) * as.numeric(target == 1L) + state$lapse * 0.5
clip_prob(p)
},
learn = function(opp_action, own_action, role_self, role_opp, payoff_opp) {
state$prev_a <- opp_action
state$prev_win <- as.integer(payoff_opp == 1L)
invisible(state)
}
)
}
opp_model_memory <- function(alpha = 0.15, tau = 8, lapse = 0.05) {
# Opponent tracks OUR action bias with leaky integration, then best-responds
state <- list(m = 0.5, alpha = alpha, tau = tau, lapse = lapse)
list(
name = "OppMemory",
init = function() { state$m <- 0.5; invisible(state) },
predict_p1 = function(role_self, role_opp) {
x <- if (role_opp == "matcher") (state$m - 0.5) else (0.5 - state$m)
p1 <- (1 - state$lapse) * sigmoid(state$tau * x) + state$lapse * 0.5
clip_prob(p1)
},
learn = function(opp_action, own_action, role_self, role_opp, payoff_opp) {
state$m <- state$m + state$alpha * (own_action - state$m)
invisible(state)
}
)
}
new_wsls_agent <- function(lapse = 0.05) {
state <- list(prev_a = NA_integer_, prev_win = NA_integer_, lapse = lapse)
list(
name = sprintf("WSLS(lapse=%.2f)", lapse),
init = function() { state$prev_a <- NA_integer_; state$prev_win <- NA_integer_; invisible(state) },
act = function(role_self, role_opp) {
if (is.na(state$prev_a)) return(sample(c(0L, 1L), 1))
if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))
if (state$prev_win == 1L) state$prev_a else (1L - state$prev_a)
},
learn = function(own_action, opp_action, payoff, role_self, role_opp) {
state$prev_a <- own_action
state$prev_win <- as.integer(payoff == 1L)
invisible(state)
}
)
}
new_memory_agent <- function(alpha = 0.15, tau = 8, lapse = 0.05) {
# Tracks opponent bias: m <- m + alpha*(opp - m)
state <- list(m = 0.5, alpha = alpha, tau = tau, lapse = lapse)
list(
name = sprintf("Memory(alpha=%.2f,tau=%.1f,lapse=%.2f)", alpha, tau, lapse),
init = function() { state$m <- 0.5; invisible(state) },
act = function(role_self, role_opp) {
if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))
x <- if (role_self == "matcher") (state$m - 0.5) else (0.5 - state$m)
rbinom(1, 1, clip_prob(sigmoid(state$tau * x)))
},
learn = function(own_action, opp_action, payoff, role_self, role_opp) {
state$m <- state$m + state$alpha * (opp_action - state$m)
invisible(state)
}
)
}
new_mixture_inference_agent <- function(
models = NULL,
lambda_logw = 0.98,
tau = 10,
lapse = 0.03
) {
# Online model averaging over opponent models (ambitious strategy)
if (is.null(models)) {
models <- list(
opp_model_random_beta(a0 = 1, b0 = 1, rho = 0.98),
opp_model_wsls(lapse = 0.10),
opp_model_memory(alpha = 0.15, tau = 8, lapse = 0.05)
)
}
state <- list(logw = rep(0, length(models)), lambda_logw = lambda_logw, tau = tau, lapse = lapse)
list(
name = "MixtureInference",
init = function() { state$logw <- rep(0, length(models)); for (m in models) m$init(); invisible(state) },
act = function(role_self, role_opp) {
if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))
p_each <- vapply(models, function(m) m$predict_p1(role_self, role_opp), numeric(1))
w <- normalize_log_weights(state$logw)
p_mix <- clip_prob(sum(w * p_each))
x <- if (role_self == "matcher") (p_mix - 0.5) else (0.5 - p_mix)
rbinom(1, 1, clip_prob(sigmoid(state$tau * x)))
},
learn = function(own_action, opp_action, payoff, role_self, role_opp) {
payoff_opp <- -payoff
p_each <- vapply(models, function(m) m$predict_p1(role_self, role_opp), numeric(1))
p_each <- clip_prob(p_each)
loglik <- if (opp_action == 1L) log(p_each) else log(1 - p_each)
state$logw <- state$lambda_logw * state$logw + loglik
for (m in models) m$learn(opp_action, own_action, role_self, role_opp, payoff_opp)
invisible(state)
}
)
}
new_random_agent <- function(theta = 0.5) {
list(
name = sprintf("Random(theta=%.2f)", theta),
init = function() invisible(NULL),
act = function(role_self, role_opp) rbinom(1, 1, theta),
learn = function(own_action, opp_action, payoff, role_self, role_opp) invisible(NULL)
)
}
simulate_match <- function(agent_A, agent_B, n_trials = 60, swap_at = 30) {
stopifnot(swap_at >= 1, swap_at <= n_trials)
agent_A$init()
agent_B$init()
role_A <- c(rep("matcher", swap_at), rep("hider", n_trials - swap_at))
role_B <- ifelse(role_A == "matcher", "hider", "matcher")
out <- vector("list", n_trials)
for (t in seq_len(n_trials)) {
a <- agent_A$act(role_self = role_A[t], role_opp = role_B[t])
b <- agent_B$act(role_self = role_B[t], role_opp = role_A[t])
payoff_A <- if (role_A[t] == "matcher") {
if (a == b) 1L else -1L
} else {
if (a != b) 1L else -1L
}
payoff_B <- -payoff_A
agent_A$learn(own_action = a, opp_action = b, payoff = payoff_A, role_self = role_A[t], role_opp = role_B[t])
agent_B$learn(own_action = b, opp_action = a, payoff = payoff_B, role_self = role_B[t], role_opp = role_A[t])
out[[t]] <- data.frame(
trial = t,
role_A = role_A[t],
role_B = role_B[t],
action_A = a,
action_B = b,
payoff_A = payoff_A,
payoff_B = payoff_B
)
}
bind_rows(out)
}
make_ordered_pairs <- function(names) {
expand.grid(A = names, B = names, stringsAsFactors = FALSE) |>
filter(A != B) |>
split(seq_len(nrow(.))) |>
lapply(\(r) list(A = r$A, B = r$B))
}
run_tournament <- function(agent_factories, n_sims = 300, n_trials = 60, swap_at = 30, seed = 1) {
set.seed(seed)
strategies <- names(agent_factories)
pairs <- make_ordered_pairs(strategies)
map_dfr(pairs, function(pair) {
Aname <- pair$A
Bname <- pair$B
pair_label <- paste0(Aname, " vs ", Bname)
map_dfr(seq_len(n_sims), function(sim_id) {
A <- agent_factories[[Aname]]()
B <- agent_factories[[Bname]]()
d <- simulate_match(A, B, n_trials = n_trials, swap_at = swap_at)
d$sim <- sim_id
d$pair <- pair_label
d$strategy_A <- Aname
d$strategy_B <- Bname
d
})
})
}
agent_factories <- list(
WSLS   = function() new_wsls_agent(lapse = 0.05),
Memory = function() new_memory_agent(alpha = 0.15, tau = 8, lapse = 0.05),
Mixture= function() new_mixture_inference_agent(lambda_logw = 0.98, tau = 10, lapse = 0.03),
Random = function() new_random_agent(theta = 0.50)
)
df <- run_tournament(agent_factories, n_sims = 300, n_trials = 60, swap_at = 30, seed = 42)
install.packages(
pkgs = "rstan",
repos = c(
"https://mc-stan.org/r-packages/",
getOption("repos")
))
rstudioapi::restartSession()
install.packages("brms")
remotes::install_github("stan-dev/cmdstanr")
install.packages("cmdstanr")
cmdstanr::check_cmdstan_toolchain(fix = TRUE)
remotes::install_github("stan-dev/cmdstanr")
remotes::install_github("stan-dev/cmdstanr")
# we recommend running this in a fresh R session or restarting your current session
install.packages("cmdstanr", repos = c('https://stan-dev.r-universe.dev', getOption("repos")))
# install.packages("remotes")
remotes::install_github("stan-dev/cmdstanr")
cmdstanr::check_cmdstan_toolchain(fix = TRUE)
remotes::install_github("stan-dev/cmdstanr")
remotes::install_github("stan-dev/cmdstanr")
install_cmdstan()
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
set.seed(42)
library(dplyr)
library(dplyr)
