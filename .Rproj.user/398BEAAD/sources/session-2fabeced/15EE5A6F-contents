# R/strategies.R
# -----------------------------------------------------------------------------
# Two strategy implementations for repeated Matching Pennies:
#   1) WSLS (Win–Stay / Lose–Shift)
#   2) k-ToM-inspired belief learning + noisy best response
# -----------------------------------------------------------------------------

# Bernoulli draw returning integer 0/1 (convenient for Matching Pennies)
rbern <- function(p) as.integer(stats::rbinom(1, size = 1, prob = p))

# Apply a lapse (random responding) to an intended action.
# Why have lapse?
# - It is a simple cognitive constraint: errors, distraction, exploration.
apply_lapse <- function(intended_action, lapse) {
  if (stats::runif(1) < lapse) return(rbern(0.5))
  intended_action
}

# --- Strategy 1: WSLS ---------------------------------------------------------
# Probabilistic WSLS lets us represent imperfections:
# - p_repeat_win  near 1 => strong win-stay
# - p_repeat_loss near 0 => strong lose-shift
# - lapse         near 0 => few random errors
make_agent_wsls <- function(params) {
  p_repeat_win  <- params$p_repeat_win
  p_repeat_loss <- params$p_repeat_loss
  lapse <- params$lapse

  state <- new.env(parent = emptyenv())
  state$last_action <- NA_integer_
  state$last_win <- NA

  list(
    name = "WSLS",

    choose = function(role, t) {
      # Trial 1: no history yet. We start unbiased.
      if (is.na(state$last_action)) return(rbern(0.5))

      # The WSLS rule depends only on the previous outcome.
      p_rep <- if (isTRUE(state$last_win)) p_repeat_win else p_repeat_loss

      repeat_action <- (stats::runif(1) < p_rep)
      intended <- if (repeat_action) state$last_action else (1L - state$last_action)

      apply_lapse(intended, lapse)
    },

    update = function(opp_action, role, win, my_action) {
      # WSLS only needs memory of 1: last action + last outcome.
      state$last_action <- my_action
      state$last_win <- win
      invisible(NULL)
    }
  )
}

# --- Strategy 2: k-ToM-inspired belief learning -------------------------------
# We implement a fully explainable "opponent model":
# - belief = P(opponent plays 1)
# - alpha controls belief updating (recency weighting)
# - beta controls how deterministic the best response is
# - lapse captures occasional random decisions
make_agent_ktom <- function(params) {
  alpha <- params$alpha
  beta  <- params$beta
  lapse <- params$lapse

  state <- new.env(parent = emptyenv())
  state$belief <- 0.5

  list(
    name = "kToM_inspired",

    choose = function(role, t) {
      # Convert belief into "value difference" between choosing 1 vs 0.
      # If Matcher: match the most likely opponent action.
      # If Mismatcher: mismatch it.
      v <- 2 * state$belief - 1
      if (role == 0L) v <- -v

      # Soft best response (logistic). Then mix with lapse.
      p1 <- (1 - lapse) * stats::plogis(beta * v) + lapse * 0.5
      rbern(p1)
    },

    update = function(opp_action, role, win, my_action) {
      # Delta-rule belief update (cognitive constraint: limited memory).
      state$belief <- state$belief + alpha * (opp_action - state$belief)
      invisible(NULL)
    }
  )
}

# Factory function used by the simulator.
make_agent <- function(strategy_name, params_row) {
  if (strategy_name == "WSLS") {
    return(make_agent_wsls(list(
      p_repeat_win  = params_row$p_repeat_win,
      p_repeat_loss = params_row$p_repeat_loss,
      lapse = params_row$lapse
    )))
  }

  if (strategy_name == "kToM") {
    return(make_agent_ktom(list(
      alpha = params_row$alpha,
      beta  = params_row$beta,
      lapse = params_row$lapse
    )))
  }

  stop("Unknown strategy: ", strategy_name)
}
