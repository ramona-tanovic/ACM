---
title: "agents"
output: html_document
---

```{r}
# Agents: WSLS, Memory, MixtureInference, Random.

new_wsls_agent <- function(lapse = 0.05) {
  state <- list(prev_a = NA_integer_, prev_win = NA_integer_, lapse = lapse)
  list(
    name = sprintf("WSLS(lapse=%.2f)", lapse),
    init = function() { state$prev_a <- NA_integer_; state$prev_win <- NA_integer_; invisible(state) },
    act = function(role_self, role_opp) {
      if (is.na(state$prev_a)) return(sample(c(0L, 1L), 1))
      if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))
      if (state$prev_win == 1L) state$prev_a else (1L - state$prev_a)
    },
    learn = function(own_action, opp_action, payoff, role_self, role_opp) {
      state$prev_a <- own_action
      state$prev_win <- as.integer(payoff == 1L)
      invisible(state)
    }
  )
}

new_memory_agent <- function(alpha = 0.15, tau = 8, lapse = 0.05) {
  state <- list(m = 0.5, alpha = alpha, tau = tau, lapse = lapse)
  list(
    name = sprintf("Memory(alpha=%.2f,tau=%.1f,lapse=%.2f)", alpha, tau, lapse),
    init = function() { state$m <- 0.5; invisible(state) },
    act = function(role_self, role_opp) {
      if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))
      x <- if (role_self == "matcher") (state$m - 0.5) else (0.5 - state$m)
      rbinom(1, 1, clip_prob(sigmoid(state$tau * x)))
    },
    learn = function(own_action, opp_action, payoff, role_self, role_opp) {
      state$m <- state$m + state$alpha * (opp_action - state$m)
      invisible(state)
    }
  )
}

new_mixture_inference_agent <- function(models = NULL, lambda_logw = 0.98, tau = 10, lapse = 0.03) {
  if (is.null(models)) {
    models <- list(
      opp_model_random_beta(),
      opp_model_wsls(lapse = 0.10),
      opp_model_memory(alpha = 0.15, tau = 8, lapse = 0.05)
    )
  }

  state <- list(logw = rep(0, length(models)), lambda_logw = lambda_logw, tau = tau, lapse = lapse)

  list(
    name = "MixtureInference",
    init = function() { state$logw <- rep(0, length(models)); for (m in models) m$init(); invisible(state) },
    act = function(role_self, role_opp) {
      if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))
      p_each <- vapply(models, function(m) m$predict_p1(role_self, role_opp), numeric(1))
      w <- normalize_log_weights(state$logw)
      p_mix <- clip_prob(sum(w * p_each))
      x <- if (role_self == "matcher") (p_mix - 0.5) else (0.5 - p_mix)
      rbinom(1, 1, clip_prob(sigmoid(state$tau * x)))
    },
    learn = function(own_action, opp_action, payoff, role_self, role_opp) {
      payoff_opp <- -payoff
      p_each <- vapply(models, function(m) m$predict_p1(role_self, role_opp), numeric(1))
      p_each <- clip_prob(p_each)
      loglik <- if (opp_action == 1L) log(p_each) else log(1 - p_each)
      state$logw <- state$lambda_logw * state$logw + loglik
      for (m in models) m$learn(opp_action, own_action, role_self, role_opp, payoff_opp)
      invisible(state)
    }
  )
}

new_random_agent <- function(theta = 0.5) {
  list(
    name = sprintf("Random(theta=%.2f)", theta),
    init = function() invisible(NULL),
    act = function(role_self, role_opp) rbinom(1, 1, theta),
    learn = function(own_action, opp_action, payoff, role_self, role_opp) invisible(NULL)
  )
}
```
