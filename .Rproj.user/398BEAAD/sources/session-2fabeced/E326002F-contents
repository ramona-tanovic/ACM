---
title: "Assignment 1 — Matching Pennies: WSLS vs k-ToM-inspired belief learning"
author: "Ramona"
format: html
execute:
  echo: false
  warning: false
  message: false
---

## What you should get from this report

This assignment asks for two things: (i) describe two plausible strategies for repeated Matching Pennies, and (ii) provide a simulation + visualization pipeline in a version-controlled repository.

The goal is not only "who wins". The goal is to connect behavior to *mechanistic* claims under cognitive constraints (limited memory, perseveration, errors/noise), and to show those mechanisms are *identifiable* via model fitting.

Course inspiration (protocol, strategies, constraints): https://fusaroli.github.io/AdvancedCognitiveModeling2023/

## Game and protocol

- Repeated Matching Pennies for **T = 60** trials.
- Each trial, one player is the **Matcher** (wants actions to match) and the other is the **Mismatcher** (wants actions to differ).
- After **30 trials** roles swap.

Why the role swap matters cognitively:

A final payoff collapses behavior into one number. The mid-game role swap changes the reward mapping (match vs mismatch). This tests whether a strategy *represents task contingencies* and re-maps behavior when contingencies flip. That is a cognitive signature, not just “performance.”

A useful reference point: in the ideal mixed-strategy equilibrium of Matching Pennies, expected payoff is 0. Persistent deviations from 0 indicate exploitability under cognitive constraints (finite memory, noise, imperfect opponent modelling).

## Strategy 1: WSLS (Win–Stay / Lose–Shift)

WSLS is a minimal, cognitively cheap strategy:

- After a win: repeat previous action (stay)
- After a loss: switch action (shift)

We implement WSLS probabilistically to encode cognitive constraints:

- `p_repeat_win`: how strongly the agent repeats after a win (perseveration / “win-stay”)
- `p_repeat_loss`: how strongly the agent repeats after a loss (failure to shift)
- `lapse`: random responding (errors / distraction)

Cognitive constraints (operational):

- Memory requirement: last action (1 bit) + last outcome (1 bit).
- Computation: one conditional probability lookup (“win?”) and a possible flip.
- Errors: implemented as `lapse`, mixing toward random responding.

Formalization (pseudo-diagram):

```text
WSLS (imperfect)

Trial 1:
  a1 ~ Bernoulli(0.5)

Trial t > 1:
  if win(t-1) == 1:
      repeat a(t-1) with prob p_repeat_win
  else:
      repeat a(t-1) with prob p_repeat_loss

  with prob lapse:
      override and choose random Bernoulli(0.5)
```

## Strategy 2: k-ToM-inspired belief learning (interpretable)

In the course notes, Theory of Mind models are about explicitly modelling what the opponent will do.

To keep the model simple and interpretable (Assignment 1 scope), we use a k-ToM-*inspired* belief-learning strategy:

- Maintain a belief \(b_t = P(\text{opponent plays }1)\)
- Update belief with a delta rule (learning rate \(\alpha\)):
  - \(b_{t+1} = b_t + \alpha\, (\text{opp}_t - b_t)\)
- Choose a role-dependent best response:
  - Matcher: match the likely opponent action
  - Mismatcher: mismatch it
- Add cognitive noise via \(\beta\) (decision sharpness) and `lapse` (random errors)

Parameters map cleanly to constraints:

- `alpha`: recency weighting / memory limitation (how quickly beliefs forget older evidence)
- `beta`: graded vs deterministic choice (noise / stochasticity)
- `lapse`: occasional random choices (errors / distraction)

Cognitive constraints (operational):

- Memory requirement: one scalar belief state \(b_t\) plus knowledge of current role.
- Computation: update a single belief and compute a best response given role.
- Errors: implemented via \(\beta\) and `lapse`.

Formalization (pseudo-diagram):

```text
Belief learning (k-ToM-inspired)

Initialize:
  belief b1 = 0.5

Trial t:
  compute value v_t from belief and role:
    if Matcher:    v_t =  2*b_t - 1
    if Mismatcher: v_t = -(2*b_t - 1)

  soft best-response:
    p(a_t = 1) = (1-lapse) * logistic(beta * v_t) + lapse * 0.5

  observe opponent action opp_t
  update belief:
    b_{t+1} = b_t + alpha * (opp_t - b_t)
```

## Where Stan is used (and why it is not bolted-on)

Who uses Stan?

- The modeller uses Stan to make assumptions explicit and to connect simulated behavior back to interpretable parameters.

What is Stan used for?

- (1) Sampling heterogeneous agent parameters from explicit priors (generative story).
- (2) Fitting hierarchical models to action sequences to infer cognitive parameters (interpretation).
- (3) Comparing competing cognitive hypotheses with out-of-sample predictive fit (LOO).

When is Stan used?

- Before simulation: to draw agent parameters from priors (fixed_param).
- After simulation: to fit models back to the generated sequences (MCMC), then compute LOO.

Where is Stan in the pipeline?

- `assignment1/stan/` contains the generative prior models and the two hierarchical fit models.

Why is this necessary (not bolted-on)?

- Because “strategy descriptions” are not enough: we also need to show that the mechanisms are (i) identifiable from finite noisy data and (ii) predictively distinct.
- Parameter recovery (simulate → fit → recover true values) is the strongest check that “the parameters mean what we claim they mean.”
- LOO makes the “two strategies” comparison explicit as competing generative hypotheses, not just a tournament.

Priors used (summary)

| Component | Parameter | Prior (generative) | Prior (fit, population-level) |
|---|---|---|---|
| WSLS | p_repeat_win | Beta(8, 2) | mu_win ~ Normal(0, 1.5) on logit scale |
| WSLS | p_repeat_loss | Beta(2, 8) | mu_loss ~ Normal(0, 1.5) on logit scale |
| WSLS | lapse | Beta(2, 10) | mu_lapse ~ Normal(-2, 1.5) on logit scale |
| Belief | alpha | Beta(2, 2) | mu_alpha ~ Normal(0, 1.5) on logit scale |
| Belief | beta | LogNormal(0, 0.5) | mu_log_beta ~ Normal(0, 1.5) on log scale |
| Belief | lapse | Beta(2, 10) | mu_lapse ~ Normal(-2, 1.5) on logit scale |

## Results

We separate three questions:

1) Performance: who tends to exploit whom (with uncertainty)?
2) Role sensitivity: what happens when contingencies flip at trial 30?
3) Mechanism: do fitted parameters recover the intended cognitive signatures, and do the models predict differently out of sample?

### Fig 1 — Tournament performance (mean payoff + uncertainty)

```{r}
knitr::include_graphics("outputs/figs/fig1_matchups.png")
```

What this figure allows you to understand:

- It gives a coarse behavioral summary: expected payoff (with bootstrap uncertainty) for each matchup.
- Because Matching Pennies has a mixed-strategy equilibrium with expected payoff 0, systematic deviations from 0 indicate exploitability under constraints (finite memory, noise, imperfect modelling).

### Fig 2 — Role swap diagnostic (contingency remapping)

```{r}
knitr::include_graphics("outputs/figs/fig2_role_advantage.png")
```

What this figure allows you to understand:

- The role swap flips the reward mapping. A strategy that explicitly represents role-contingencies should show systematic differences between being Matcher and being Mismatcher.
- A useful cognitive interpretation is “remapping”: after trial 30, does the policy quickly re-align with the new utility mapping, or does it show inertia/lag?

### Fig 3 — Main interpretation figure (Chapter-5 style): posterior + recovery

This is the key mechanism figure.

- Top panels: posterior densities for a few example subjects with the true generating value (dashed line).
- Bottom panels: recovery scatter (true value vs posterior mean ± 95% interval).

```{r}
knitr::include_graphics("outputs/figs/fig3_interpretation_combined.png")
```

What you conclude from this plot:

- Identifiability: posteriors concentrate around the true generating values for key parameters, implying the mechanisms are recoverable from sequences of length T=60 (given these priors).
- Cognitive signatures:
  - WSLS: `p_repeat_win` vs `p_repeat_loss` expresses reinforcement-like “stay/shift” asymmetry; `lapse` captures errors.
  - Belief learning: `alpha` expresses memory/recency weighting; `beta` expresses graded choice; `lapse` captures errors.

What would count as a failure:

- Broad posteriors that do not concentrate near the true values (or intervals that systematically miss) would suggest the parameters are not identified at this trial length, or that priors dominate the likelihood.

Note on recovery plots:

- Recovery scatter is most diagnostic when true values span a wide range. If true values are clustered (by design or by narrow priors), posterior concentration in the density panels carries more interpretive weight than the slope of the recovery scatter.

### Fig 4 — Payoff distributions (variance / robustness)

```{r}
knitr::include_graphics("outputs/figs/fig4_payoff_distribution.png")
```

What this figure allows you to understand:

- Mean payoffs can hide instability. Distributions show whether a strategy’s “advantage” is robust or driven by a few extreme simulations.

### LOO model comparison (table + dot)

We fit both hierarchical models to both datasets and compare out-of-sample predictive fit (LOO).

How to read this:

- Reported quantities are differences in expected log predictive density (elpd) with a standard error (SE).
- If an elpd difference is small relative to its SE, evidence for a better model is weak at this sample size / noise level.

```{r}
library(readr)
loo_tbl <- read_csv("outputs/data/loo_comparison_table.csv", show_col_types = FALSE)
loo_tbl
```

```{r}
knitr::include_graphics("outputs/figs/fig5_loo_dot.png")
```

What this allows you to understand:

- Whether the WSLS model predicts WSLS-generated behavior better than the belief model, and vice versa.
- This turns “two strategies” into explicit competing generative hypotheses, beyond payoff.

## Link to repository

Add your GitHub/GitLab link here once you push the repo.

## Short conclusion template

WSLS and k-ToM-inspired belief learning differ not only in average payoff, but in contingency remapping after the role swap and in parameter-level signatures under Stan model fits; parameter recovery and LOO comparisons support the interpretation that the fitted mechanisms map onto cognitive constraints (perseveration, memory limitation, and errors), rather than being descriptive-only.
