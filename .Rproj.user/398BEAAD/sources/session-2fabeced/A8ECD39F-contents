# Strategies are agent objects with init/act/learn.
# WSLS and ForgetfulBias are parameterised, and each agent samples its parameters
# from Stan prior draws at spawn time.

make_random <- function(p = 0.5) {
  list(
    name = sprintf("Random(%.1f)", p),
    spawn = function() {
      state <- new.env(parent = emptyenv())
      list(
        init = function(role) { state$role <- role },
        act = function(ctx) { rbinom(1, 1, p) },
        learn = function(ctx, my_action, opp_action, payoff) { invisible(NULL) }
      )
    }
  )
}

make_wsls_stan <- function(wsls_draws) {
  list(
    name = "WSLS(Stan)",
    spawn = function() {
      # Sample one parameter draw for this agent
      row <- wsls_draws[sample.int(nrow(wsls_draws), 1), , drop = FALSE]
      p_win <- as.numeric(row$p_repeat_win)
      p_loss <- as.numeric(row$p_repeat_loss)
      lapse <- as.numeric(row$lapse)
      
      state <- new.env(parent = emptyenv())
      state$role <- NA_character_
      
      list(
        init = function(role) { state$role <- role },
        
        act = function(ctx) {
          # First trial: no history -> random
          if (is.na(ctx$my_last) || is.na(ctx$last_win)) {
            return(rbinom(1, 1, 0.5))
          }
          
          # Base repeat probability depends on whether last trial was a win
          base_p <- ifelse(ctx$last_win == 1, p_win, p_loss)
          
          # Lapse mixes with random 0.5
          p_repeat <- (1 - lapse) * base_p + lapse * 0.5
          
          # Sample whether we repeat
          do_repeat <- rbinom(1, 1, p_repeat)
          
          if (do_repeat == 1) {
            return(ctx$my_last)
          } else {
            return(1 - ctx$my_last)
          }
        },
        
        learn = function(ctx, my_action, opp_action, payoff) { invisible(NULL) }
      )
    }
  )
}

make_forgetful_bias_stan <- function(bias_draws) {
  list(
    name = "ForgetfulBias(Stan)",
    spawn = function() {
      row <- bias_draws[sample.int(nrow(bias_draws), 1), , drop = FALSE]
      eta <- as.numeric(row$eta)
      lapse <- as.numeric(row$lapse)

      state <- new.env(parent = emptyenv())
      state$role <- NA_character_
      state$p_hat <- 0.5

      list(
        init = function(role) { state$role <- role },
        act = function(ctx) {
          if (runif(1) < lapse) return(rbinom(1, 1, 0.5))

          pred_opp <- ifelse(state$p_hat >= 0.5, 1, 0)
          if (state$role == "Matcher") pred_opp else (1 - pred_opp)
        },
        learn = function(ctx, my_action, opp_action, payoff) {
          state$p_hat <- (1 - eta) * state$p_hat + eta * opp_action
          invisible(NULL)
        }
      )
    }
  )
}
