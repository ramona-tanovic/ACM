# R/config.R
# -----------------------------------------------------------------------------
# Central configuration for the assignment.
# -----------------------------------------------------------------------------

# --- Simulation settings ------------------------------------------------------

T_TRIALS <- 60          # Number of trials in one repeated game (per match)
HALF_TRIAL <- T_TRIALS / 2  # Role swap happens after HALF_TRIAL
N_SIMS <- 1000          # Tournament simulations per ordered pairing (A vs B)
SEED <- 123             # Seed for reproducibility

# --- What we treat as "payoff" -----------------------------------------------
# We use a symmetric zero-sum payoff:
#   win  = +1
#   loss = -1
# This is convenient because:
# - expected payoff is centered at 0 for "fair" play
# - differences are interpretable as advantage/disadvantage
PAYOFF_WIN <-  1
PAYOFF_LOSS <- -1

# --- Bootstrap uncertainty ----------------------------------------------------
# We show uncertainty intervals by bootstrapping across simulations.
BOOT_R <- 500          # Number of bootstrap resamples for each summary
CI_LEVEL <- 0.95       # 95% intervals (typical)

# --- Strategy set -------------------------------------------------------------
# The order here defines how strategies appear in plots and tables.
STRATEGIES <- c("StanRandom", "WSLS", "kToM")

# --- Priors (Stan fixed_param) -----------------------------------------------
# These priors generate *heterogeneous agents*.
# Each match gets a fresh draw of parameters for each player.

# StanRandom: action bias p ~ Beta(a, b)
PRIOR_RANDOM <- list(a = 2, b = 2)  # centered at 0.5, mild variance

# WSLS: repeat probabilities + lapse
PRIOR_WSLS <- list(
  a_win  = 8, b_win  = 2,   # p_repeat_win tends high
  a_loss = 2, b_loss = 8,   # p_repeat_loss tends low (i.e., shift after loss)
  a_lapse = 2, b_lapse = 10 # mostly low lapse (mostly not random)
)

# k-ToM-ish: belief update strength + decision sharpness + lapse
# - alpha in (0,1): how quickly beliefs track opponent actions
# - beta > 0: inverse temperature (higher = more deterministic)
# - lapse in (0,1): mixture toward 0.5 random responding
PRIOR_KTOM <- list(
  a_alpha = 2, b_alpha = 2,      # moderate learning rate
  mu_log_beta = 0.0, sigma_log_beta = 0.5,  # lognormal for beta
  a_lapse = 2, b_lapse = 10
)

# Depth parameter for k-ToM (kept fixed for simplicity and interpretability)
KTOM_K <- 2

# --- Policy-model fitting settings -------------------------------------------
# We fit an interpretable WSLS policy model (wsls_policy.stan) to sequences
# generated by each strategy playing against StanRandom.

S_FIT <- 200    # number of independent matches used for the fit
STAN_CHAINS <- 4
STAN_WARMUP <- 500
STAN_SAMPLES <- 500

# --- Output paths -------------------------------------------------------------
OUT_DATA_DIR <- file.path("outputs", "data")
OUT_FIG_DIR  <- file.path("outputs", "figs")

# Utility: make sure output folders exist
dir.create(OUT_DATA_DIR, recursive = TRUE, showWarnings = FALSE)
dir.create(OUT_FIG_DIR,  recursive = TRUE, showWarnings = FALSE)
