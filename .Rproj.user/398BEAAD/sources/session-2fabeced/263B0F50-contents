// stan/wsls_policy.stan
// -----------------------------------------------------------------------------
// Interpretable WSLS policy model for action sequences.
//
// Data are many independent matches concatenated into one long vector.
// We only evaluate transitions *within* a match (t=2..T for each match).
//
// Parameters (all in [0,1]):
// - p_repeat_win  : probability of repeating previous action after a win
// - p_repeat_loss : probability of repeating previous action after a loss
// - lapse         : mixture toward random responding (0.5)
//
// Likelihood idea:
// Let same_t = 1 if a_t == a_{t-1}, else 0.
// Then:
//   P(same_t = 1) = (1-lapse) * p_rep + lapse * 0.5
// where p_rep = p_repeat_win if previous trial was a win, else p_repeat_loss.
//
// This is deliberately simple so the posterior is easy to interpret.
// -----------------------------------------------------------------------------
data {
  int<lower=1> S;                     // number of sequences (matches)
  int<lower=2> T;                     // trials per sequence

  array[S * T] int<lower=0, upper=1> a;      // A actions (0/1), concatenated
  array[S * T] int<lower=0, upper=1> b;      // B actions (0/1), concatenated
  array[S * T] int<lower=0, upper=1> roleA;  // 1 = Matcher, 0 = Mismatcher
}

transformed data {
  array[S * T] int<lower=0, upper=1> winA;

  for (i in 1:(S * T)) {
    int match = (a[i] == b[i]);       // 1 if same action, else 0
    if (roleA[i] == 1) {
      winA[i] = match;                // Matcher wins on match
    } else {
      winA[i] = 1 - match;            // Mismatcher wins on mismatch
    }
  }
}

parameters {
  real<lower=0, upper=1> p_repeat_win;
  real<lower=0, upper=1> p_repeat_loss;
  real<lower=0, upper=1> lapse;
}

model {
  // mild, regularizing priors (tweak if you want)
  p_repeat_win  ~ beta(2, 2);
  p_repeat_loss ~ beta(2, 2);
  lapse         ~ beta(2, 8);

  // likelihood: for each sequence, trials 2..T depend on previous win/loss
  for (s in 1:S) {
    int base = (s - 1) * T;

    for (t in 2:T) {
      int idx  = base + t;
      int prev = base + (t - 1);

      real p_rep  = (winA[prev] == 1) ? p_repeat_win : p_repeat_loss;
      real p_same = (1 - lapse) * p_rep + lapse * 0.5;

      target += bernoulli_lpmf( (a[idx] == a[prev]) | p_same );
    }
  }
}

generated quantities {
  // convenience: implied "WSLS strength" summary
  real wsls_delta = p_repeat_win - p_repeat_loss;
}
