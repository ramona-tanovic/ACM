# R/strategies.R
# -----------------------------------------------------------------------------
# Strategy implementations for repeated Matching Pennies.
#
# Two strategies (as requested):
#  1) WSLS  : Win–Stay / Lose–Shift ("immediate reaction")
#  2) k-ToM : an interpretable "Theory of Mind"-flavoured model
#
# Each strategy is a small *stateful* object with:
#  - choose(role, t)  -> returns action 0/1
#  - update(...)      -> updates internal state for the next trial
#
# This is deliberately close to cognitive language:
# a strategy is a policy + a memory state + (optional) noise.
# -----------------------------------------------------------------------------

# Sample a Bernoulli(p) action returning 0/1
rbern <- function(p) {
  as.integer(stats::rbinom(1, size = 1, prob = p))
}

# "Lapse" = probability of a random choice (models errors / distraction)
apply_lapse <- function(action01, lapse) {
  if (stats::runif(1) < lapse) return(rbern(0.5))
  action01
}

# --- WSLS (Win–Stay / Lose–Shift) --------------------------------------------
# Verbal rule (course notes):
# - if you won last trial, repeat the previous action
# - if you lost last trial, switch to the other action
#
# We implement a *probabilistic* WSLS:
# - p_repeat_win  : repeat after win with this probability
# - p_repeat_loss : repeat after loss with this probability
# - lapse         : on some trials, ignore WSLS and respond randomly
#
# This matches the cognitive-constraints discussion: errors and imperfect rules.
make_agent_wsls <- function(params) {

  p_repeat_win  <- params$p_repeat_win
  p_repeat_loss <- params$p_repeat_loss
  lapse <- params$lapse

  # Internal memory state (only 1 previous action + previous outcome)
  state <- new.env(parent = emptyenv())
  state$last_action <- NA_integer_
  state$last_win <- NA

  list(
    name = "WSLS",

    choose = function(role, t) {
      # Trial 1 has no previous outcome; start unbiased.
      if (is.na(state$last_action)) return(rbern(0.5))

      p_rep <- if (isTRUE(state$last_win)) p_repeat_win else p_repeat_loss
      repeat_action <- (stats::runif(1) < p_rep)

      intended <- if (repeat_action) state$last_action else (1L - state$last_action)
      apply_lapse(intended, lapse)
    },

    update = function(opp_action, role, win, my_action) {
      state$last_action <- my_action
      state$last_win <- win
      invisible(NULL)
    }
  )
}

# --- k-ToM (interpretable belief learning + best response) --------------------
# We avoid a black-box ToM implementation. Instead we implement something you
# can explain clearly in the report:
#
# 1) The agent maintains a belief b_opp = P(opponent plays action 1).
# 2) After seeing the opponent's action, it updates b_opp with a delta rule
#    controlled by alpha (0..1). Higher alpha means "faster updating" and
#    effectively *shorter memory*.
# 3) It chooses a best response given its current role:
#    - if Matcher: match the likely opponent action
#    - if Mismatcher: mismatch the likely opponent action
# 4) Choice is noisy via a soft rule controlled by beta (>0) and a lapse.
#
# Where is the "ToM" idea?
# - The agent is explicitly modelling the opponent (not just updating action
#   values). This is the core jump from simple reinforcement to ToM-style models.
make_agent_ktom <- function(params) {

  alpha <- params$alpha
  beta  <- params$beta
  lapse <- params$lapse

  state <- new.env(parent = emptyenv())
  state$b_opp <- 0.5

  # Convert belief into probability of choosing action 1.
  # role: 1 = Matcher, 0 = Mismatcher
  p_choose1 <- function(b_opp, role, beta, lapse) {
    # v in [-1,1] captures "how much" opponent seems to prefer action 1
    v <- (2 * b_opp - 1)
    if (role == 0L) v <- -v  # flip preference if Mismatcher

    # Soft decision: logistic around 0.5. Higher beta = sharper choices.
    p1 <- stats::plogis(beta * 2 * v)

    # Lapse mixture towards random responding
    (1 - lapse) * p1 + lapse * 0.5
  }

  list(
    name = "kToM",

    choose = function(role, t) {
      p1 <- p_choose1(state$b_opp, role = role, beta = beta, lapse = lapse)
      rbern(p1)
    },

    update = function(opp_action, role, win, my_action) {
      # Delta-rule belief update: exponential recency weighting.
      state$b_opp <- state$b_opp + alpha * (opp_action - state$b_opp)
      invisible(NULL)
    }
  )
}

# --- Factory: create an agent given strategy name + parameter row -------------
make_agent <- function(strategy_name, params_row) {
  if (strategy_name == "WSLS") {
    return(make_agent_wsls(list(
      p_repeat_win  = params_row$p_repeat_win,
      p_repeat_loss = params_row$p_repeat_loss,
      lapse = params_row$lapse
    )))
  }
  if (strategy_name == "kToM") {
    return(make_agent_ktom(list(
      alpha = params_row$alpha,
      beta  = params_row$beta,
      lapse = params_row$lapse
    )))
  }
  stop("Unknown strategy: ", strategy_name)
}

# --- Prior samplers (for heterogeneous agents) --------------------------------
# These functions are used by the tournament runner.

sample_params_wsls <- function(n, prior = PRIOR_WSLS) {
  data.frame(
    p_repeat_win  = stats::rbeta(n, prior$p_repeat_win[1],  prior$p_repeat_win[2]),
    p_repeat_loss = stats::rbeta(n, prior$p_repeat_loss[1], prior$p_repeat_loss[2]),
    lapse         = stats::rbeta(n, prior$lapse[1],         prior$lapse[2])
  )
}

sample_params_ktom <- function(n, prior = PRIOR_KTOM) {
  data.frame(
    alpha = stats::rbeta(n, prior$alpha[1], prior$alpha[2]),
    beta  = stats::rlnorm(n, meanlog = prior$beta[1], sdlog = prior$beta[2]),
    lapse = stats::rbeta(n, prior$lapse[1], prior$lapse[2])
  )
}
