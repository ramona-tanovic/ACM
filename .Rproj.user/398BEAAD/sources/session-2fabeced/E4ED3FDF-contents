# Strategy specs: each spec has a name and a spawn() function returning an agent with
# init(role), act(ctx), learn(ctx, my_action, opp_action, payoff).

make_stan_random <- function(rand_draws) {
  list(
    name = "StanRandom",
    spawn = function() {
      row <- rand_draws[sample.int(nrow(rand_draws), 1), , drop = FALSE]
      p <- as.numeric(row$p)

      state <- new.env(parent = emptyenv())
      state$role <- NA_character_

      list(
        init = function(role) { state$role <- role },
        act = function(ctx) { rbinom(1, 1, p) },
        learn = function(ctx, my_action, opp_action, payoff) invisible(NULL)
      )
    }
  )
}

make_wsls_stan <- function(wsls_draws) {
  list(
    name = "WSLS(Stan)",
    spawn = function() {
      row <- wsls_draws[sample.int(nrow(wsls_draws), 1), , drop = FALSE]
      p_win <- as.numeric(row$p_repeat_win)
      p_loss <- as.numeric(row$p_repeat_loss)
      lapse <- as.numeric(row$lapse)

      state <- new.env(parent = emptyenv())
      state$role <- NA_character_

      list(
        init = function(role) { state$role <- role },

        act = function(ctx) {
          # First trial: no history -> random
          if (is.na(ctx$my_last) || is.na(ctx$last_win)) {
            return(rbinom(1, 1, 0.5))
          }

          base_p <- ifelse(ctx$last_win == 1, p_win, p_loss)
          p_repeat <- (1 - lapse) * base_p + lapse * 0.5

          do_repeat <- rbinom(1, 1, p_repeat)
          if (do_repeat == 1) ctx$my_last else (1 - ctx$my_last)
        },

        learn = function(ctx, my_action, opp_action, payoff) invisible(NULL)
      )
    }
  )
}

# k-ToM: mixture over levels 0..k, with Bayesian-ish weight updates based on predictive accuracy.
# Level-0 predicts opponent = 0/1 with p=0.5.
# Higher levels best-respond to lower levels given role (Matcher vs Mismatcher).
# This is a pragmatic tournament-ready implementation (not a full generative ToM model).

make_ktom_stan <- function(ktom_draws, k = 2) {
  stopifnot(k >= 1)

  list(
    name = paste0("kToM(Stan,k=", k, ")"),
    spawn = function() {
      row <- ktom_draws[sample.int(nrow(ktom_draws), 1), , drop = FALSE]
      eta <- as.numeric(row$eta)
      beta <- as.numeric(row$beta)

      # w_raw is stored as w_raw[1], w_raw[2], ... in the draws df
      w_cols <- grep("^w_raw\\[", names(row), value = TRUE)
      w_raw <- as.numeric(row[1, w_cols, drop = TRUE])
      w_raw <- w_raw[seq_len(k + 1)]  # levels 0..k

      # Softmax to mixture weights
      w <- exp(w_raw - max(w_raw))
      w <- w / sum(w)

      state <- new.env(parent = emptyenv())
      state$role <- NA_character_
      state$w <- w
      state$opp_last <- NA_integer_

      # Predict opponent action for each level
      pred_opp_levels <- function() {
        preds <- rep(0.5, k + 1)
        preds[1] <- 0.5  # level 0

        # deterministic alternating best-response in matching pennies:
        # if you believe opp plays 1 with prob q:
        # Matcher best-responds by playing 1 if q>0.5 else 0 (ties random).
        # Mismatcher best-responds by playing 0 if q>0.5 else 1.
        # Here levels>0 predict opponent as the *action* produced by lower level best-response.
        q <- 0.5
        for (lev in 2:(k + 1)) {
          # lower level's predicted action of "self" (from opp perspective)
          # We treat it as opponent using level-(lev-2) reasoning; for a tournament heuristic, recurse action not full belief.
          if (state$role == "Matcher") {
            a_self <- ifelse(q > 0.5, 1, ifelse(q < 0.5, 0, rbinom(1,1,0.5)))
          } else {
            a_self <- ifelse(q > 0.5, 0, ifelse(q < 0.5, 1, rbinom(1,1,0.5)))
          }
          preds[lev] <- a_self
          q <- preds[lev]  # collapse to degenerate belief for next step
        }
        preds
      }

      list(
        init = function(role) { state$role <- role },

        act = function(ctx) {
          preds <- pred_opp_levels()
          # mixture prediction of opponent action (prob of 1)
          q_hat <- sum(state$w * preds)

          # Soft best-response using logistic around 0.5
          # Matcher: play 1 when q_hat high; Mismatcher: play 1 when q_hat low.
          if (state$role == "Matcher") {
            p1 <- plogis(beta * (q_hat - 0.5))
          } else {
            p1 <- plogis(beta * (0.5 - q_hat))
          }

          rbinom(1, 1, p1)
        },

        learn = function(ctx, my_action, opp_action, payoff) {
          preds <- pred_opp_levels()

          # Update mixture weights by how well each level predicted opp_action
          # Likelihood: Bernoulli(pred)
          lik <- preds * opp_action + (1 - preds) * (1 - opp_action)
          lik <- pmax(lik, 1e-6)  # avoid zeros

          w_new <- state$w * lik
          w_new <- w_new / sum(w_new)

          # Forgetting / learning-rate blend
          state$w <- (1 - eta) * state$w + eta * w_new
          state$w <- state$w / sum(state$w)

          invisible(NULL)
        }
      )
    }
  )
}
