"0","opp_model_random_beta <- function(a0 = 1, b0 = 1, rho = 0.98) {"
"0","  # Leaky Beta-Bernoulli counts for P(opp=1)"
"0","  state <- list(a = a0, b = b0, a0 = a0, b0 = b0, rho = rho)"
"0",""
"0","  list("
"0","    name = ""OppRandomBeta"","
"0","    init = function() { state$a <- state$a0; state$b <- state$b0; invisible(state) },"
"0","    predict_p1 = function(role_self, role_opp) clip_prob(state$a / (state$a + state$b)),"
"0","    learn = function(opp_action, own_action, role_self, role_opp, payoff_opp) {"
"0","      state$a <- state$a0 + state$rho * (state$a - state$a0) + opp_action"
"0","      state$b <- state$b0 + state$rho * (state$b - state$b0) + (1 - opp_action)"
"0","      invisible(state)"
"0","    }"
"0","  )"
"0","}"
"0",""
"0","opp_model_wsls <- function(lapse = 0.10) {"
"0","  # Opponent-as-WSLS with lapse"
"0","  state <- list(prev_a = NA_integer_, prev_win = NA_integer_, lapse = lapse)"
"0",""
"0","  list("
"0","    name = ""OppWSLS"","
"0","    init = function() { state$prev_a <- NA_integer_; state$prev_win <- NA_integer_; invisible(state) },"
"0","    predict_p1 = function(role_self, role_opp) {"
"0","      if (is.na(state$prev_a)) return(0.5)"
"0","      target <- if (state$prev_win == 1L) state$prev_a else (1L - state$prev_a)"
"0","      p <- (1 - state$lapse) * as.numeric(target == 1L) + state$lapse * 0.5"
"0","      clip_prob(p)"
"0","    },"
"0","    learn = function(opp_action, own_action, role_self, role_opp, payoff_opp) {"
"0","      state$prev_a <- opp_action"
"0","      state$prev_win <- as.integer(payoff_opp == 1L)"
"0","      invisible(state)"
"0","    }"
"0","  )"
"0","}"
"0",""
"0","opp_model_memory <- function(alpha = 0.15, tau = 8, lapse = 0.05) {"
"0","  # Opponent tracks OUR action bias with leaky integration, then best-responds"
"0","  state <- list(m = 0.5, alpha = alpha, tau = tau, lapse = lapse)"
"0",""
"0","  list("
"0","    name = ""OppMemory"","
"0","    init = function() { state$m <- 0.5; invisible(state) },"
"0","    predict_p1 = function(role_self, role_opp) {"
"0","      x <- if (role_opp == ""matcher"") (state$m - 0.5) else (0.5 - state$m)"
"0","      p1 <- (1 - state$lapse) * sigmoid(state$tau * x) + state$lapse * 0.5"
"0","      clip_prob(p1)"
"0","    },"
"0","    learn = function(opp_action, own_action, role_self, role_opp, payoff_opp) {"
"0","      state$m <- state$m + state$alpha * (own_action - state$m)"
"0","      invisible(state)"
"0","    }"
"0","  )"
"0","}"
