"0","new_wsls_agent <- function(lapse = 0.05) {"
"0","  state <- list(prev_a = NA_integer_, prev_win = NA_integer_, lapse = lapse)"
"0",""
"0","  list("
"0","    name = sprintf(""WSLS(lapse=%.2f)"", lapse),"
"0","    init = function() { state$prev_a <- NA_integer_; state$prev_win <- NA_integer_; invisible(state) },"
"0","    act = function(role_self, role_opp) {"
"0","      if (is.na(state$prev_a)) return(sample(c(0L, 1L), 1))"
"0","      if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))"
"0","      if (state$prev_win == 1L) state$prev_a else (1L - state$prev_a)"
"0","    },"
"0","    learn = function(own_action, opp_action, payoff, role_self, role_opp) {"
"0","      state$prev_a <- own_action"
"0","      state$prev_win <- as.integer(payoff == 1L)"
"0","      invisible(state)"
"0","    }"
"0","  )"
"0","}"
"0",""
"0","new_memory_agent <- function(alpha = 0.15, tau = 8, lapse = 0.05) {"
"0","  # Tracks opponent bias: m <- m + alpha*(opp - m)"
"0","  state <- list(m = 0.5, alpha = alpha, tau = tau, lapse = lapse)"
"0",""
"0","  list("
"0","    name = sprintf(""Memory(alpha=%.2f,tau=%.1f,lapse=%.2f)"", alpha, tau, lapse),"
"0","    init = function() { state$m <- 0.5; invisible(state) },"
"0","    act = function(role_self, role_opp) {"
"0","      if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))"
"0","      x <- if (role_self == ""matcher"") (state$m - 0.5) else (0.5 - state$m)"
"0","      rbinom(1, 1, clip_prob(sigmoid(state$tau * x)))"
"0","    },"
"0","    learn = function(own_action, opp_action, payoff, role_self, role_opp) {"
"0","      state$m <- state$m + state$alpha * (opp_action - state$m)"
"0","      invisible(state)"
"0","    }"
"0","  )"
"0","}"
"0",""
"0","new_mixture_inference_agent <- function("
"0","  models = NULL,"
"0","  lambda_logw = 0.98,"
"0","  tau = 10,"
"0","  lapse = 0.03"
"0",") {"
"0","  # Online model averaging over opponent models (ambitious strategy)"
"0","  if (is.null(models)) {"
"0","    models <- list("
"0","      opp_model_random_beta(a0 = 1, b0 = 1, rho = 0.98),"
"0","      opp_model_wsls(lapse = 0.10),"
"0","      opp_model_memory(alpha = 0.15, tau = 8, lapse = 0.05)"
"0","    )"
"0","  }"
"0",""
"0","  state <- list(logw = rep(0, length(models)), lambda_logw = lambda_logw, tau = tau, lapse = lapse)"
"0",""
"0","  list("
"0","    name = ""MixtureInference"","
"0","    init = function() { state$logw <- rep(0, length(models)); for (m in models) m$init(); invisible(state) },"
"0","    act = function(role_self, role_opp) {"
"0","      if (runif(1) < state$lapse) return(sample(c(0L, 1L), 1))"
"0","      p_each <- vapply(models, function(m) m$predict_p1(role_self, role_opp), numeric(1))"
"0","      w <- normalize_log_weights(state$logw)"
"0","      p_mix <- clip_prob(sum(w * p_each))"
"0","      x <- if (role_self == ""matcher"") (p_mix - 0.5) else (0.5 - p_mix)"
"0","      rbinom(1, 1, clip_prob(sigmoid(state$tau * x)))"
"0","    },"
"0","    learn = function(own_action, opp_action, payoff, role_self, role_opp) {"
"0","      payoff_opp <- -payoff"
"0","      p_each <- vapply(models, function(m) m$predict_p1(role_self, role_opp), numeric(1))"
"0","      p_each <- clip_prob(p_each)"
"0","      loglik <- if (opp_action == 1L) log(p_each) else log(1 - p_each)"
"0","      state$logw <- state$lambda_logw * state$logw + loglik"
"0","      for (m in models) m$learn(opp_action, own_action, role_self, role_opp, payoff_opp)"
"0","      invisible(state)"
"0","    }"
"0","  )"
"0","}"
"0",""
"0","new_random_agent <- function(theta = 0.5) {"
"0","  list("
"0","    name = sprintf(""Random(theta=%.2f)"", theta),"
"0","    init = function() invisible(NULL),"
"0","    act = function(role_self, role_opp) rbinom(1, 1, theta),"
"0","    learn = function(own_action, opp_action, payoff, role_self, role_opp) invisible(NULL)"
"0","  )"
"0","}"
